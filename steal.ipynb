{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b32258",
   "metadata": {},
   "source": [
    "# Double Dueling Deep Q-Network (DQN) using PyTorch\n",
    "Environment: LunarLander-v2\n",
    "\n",
    "### Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "555ed7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312da450",
   "metadata": {},
   "source": [
    "## Specify the Environment, and Explore the State and Action Spaces\n",
    "* Let's begin with an initializing the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fa0d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space:  Box(8,)\n",
      "State shape:  (8,)\n",
      "Action space:  Discrete(4)\n",
      "Number of actions:  4\n"
     ]
    }
   ],
   "source": [
    "# Create an environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(0);\n",
    "print('State space: ', env.observation_space)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "\n",
    "print('Action space: ', env.action_space)\n",
    "print('Number of actions: ', env.action_space.n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b9996",
   "metadata": {},
   "source": [
    "### Implement Q-Network\n",
    "Building the Network: Actor (policy) Model\n",
    "input_size = state_size\n",
    "output_size = action_size\n",
    "using same seed\n",
    "hidden_layers: fc1, fc2\n",
    "\n",
    "<!-- Define Layer of model: [FC-RELU-FC-RELU-FC] -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "514d85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        '''\n",
    "        Builds a feedforward nework with two hidden layers\n",
    "        Initialize parameters\n",
    "        \n",
    "        Params\n",
    "        =========\n",
    "        state_size (int): Dimension of each state (input_size)\n",
    "        action_size (int): dimension of each action (output_size)\n",
    "        seed (int): Random seed(using 0)\n",
    "        fc1_units (int): Size of the first hidden layer\n",
    "        fc2_units (int): Size of the second hidden layer\n",
    "        '''\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        # Add the first laer, input to hidden layer\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        # Add more hidden layer\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "\n",
    "        # State-value V\n",
    "        self.V = nn.Linear(fc2_units, 1)\n",
    "        \n",
    "        # Advantage function A\n",
    "        self.A = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass through the network. Build a network that mps state -> action values.\n",
    "        \n",
    "        return Q function\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        V = self.V(x)\n",
    "        A = self.A(x)\n",
    "        \n",
    "        return V + (A - A.mean(dim=1, keepdim=True))\n",
    "    \n",
    "    \n",
    "# with out Dueling\n",
    "# class QNetwork(nn.Module):\n",
    "#     def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "#         '''\n",
    "#         Builds a feedforward nework with two hidden layers\n",
    "#         Initialize parameters\n",
    "        \n",
    "#         Params\n",
    "#         =========\n",
    "#         state_size (int): Dimension of each state (input_size)\n",
    "#         action_size (int): dimension of each action (output_size)\n",
    "#         seed (int): Random seed(using 0)\n",
    "#         fc1_units (int): Size of the first hidden layer\n",
    "#         fc2_units (int): Size of the second hidden layer\n",
    "#         '''\n",
    "#         super(QNetwork, self).__init__()\n",
    "#         self.seed = torch.manual_seed(seed)\n",
    "#         # Add the first laer, input to hidden layer\n",
    "#         self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "#         # Add more hidden layer\n",
    "#         self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "#         self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "#     def forward(self, state):\n",
    "#         \"\"\"\n",
    "#         Forward pass through the network. Build a network that mps state -> action values.\n",
    "#         \"\"\"\n",
    "#         x = F.relu(self.fc1(state))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         return self.fc3(x)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d9ceab",
   "metadata": {},
   "source": [
    "# Experience Replay\n",
    "\n",
    "### Set finite memory size N\n",
    "Algorithm only stores the last N experience tuples in the replay memory, and sample uniformly at random from D when performing updates. The memory buffer does not differentiate imporant transitions and always overwrites with recent transitions owing to the finitre memory size N.\n",
    "\n",
    "Uniform sampling gives equal importance to all transitions in the replay memory\n",
    "\n",
    "We store each experienced tuple in the buffer as we are interacting with the environment and then sample a small bath of tuples from it in order to learn. Therefore, we are able to learn from individual tuples multiple times\n",
    "\n",
    "Sequential order runs the risk of getting swayed by the effect of the correlations.\n",
    "\n",
    "With experience replay, can sample from this buffer at random\n",
    "\n",
    "Randomizing the samples breaks these correlations and therefore reduces the variance of the updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "189dba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        '''\n",
    "        Only stroes the last N experience tuples in the replay memory\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        '''\n",
    "        # Initialize replay memory\n",
    "        self.acion_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size) # set N memory size\n",
    "        self.batch_size = batch_size\n",
    "        # build named experience tuples\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        '''\n",
    "        we store the agent's experiences at each time-step, e_t = (s_t,a_t,r_t,s_(t+1))\n",
    "        '''\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        '''\n",
    "        Samples uniformly at random from D(D_t = {e_1,...,e_t}) when  performing updates\n",
    "        '''\n",
    "        # D\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        #store in\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device) # gpu\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        # return D\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Return the current size of internal memory\n",
    "        '''\n",
    "        return len(self.memory)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbcb7fe",
   "metadata": {},
   "source": [
    "### Implement agent\n",
    "* Agent(state_size=8, action_size=4, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd74428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# hyperparameters\n",
    "LR = 5e-4                # learning rate\n",
    "BUFFER_SIZE = int(1e5)   # replay buffer size N\n",
    "BATCH_SIZE = 64          # minibatch size\n",
    "UPDATE_EVERY = 4         # how often to update the network\n",
    "GAMMA = 0.99             # Discount factor\n",
    "TAU = 1e-3               # for soft update of target parameters\n",
    "\n",
    "\n",
    "# Setup Gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Build Agent(): Evaluate our agent on unmodified games (dqn agent)\n",
    "class Agent():\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, seed): #8, 4, 0\n",
    "        \"\"\"\n",
    "        Initialize an Agent object.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state = 8\n",
    "            action_size (int): dimension of each action = 4\n",
    "            seed (int): random seed = 0\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # Q-Network: Neural network function approx. with weights theta θ as a Q-Network.\n",
    "        # A Q-Network can be trained by adjusting the parameters θ_i at iteration i to reduce the mse in the Bellman equation\n",
    "        # The outputs correspond to the predicted Q-values of the individual action for input state\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device) # gpu\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        # specify optimizer(Adam)\n",
    "        # optim.Adam(Qnet.parameters(), small learning rate)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR) ###\n",
    "        \n",
    "        # First, use a technique known as experience replay in which we stre the agent's experience at each time-step,\n",
    "        # e_t= (s_t, a_t, r_t, s_(t_1)), in a data set D_t ={e_1,...,e_t},pooled over many episodes(where the end of an episode occurs when\n",
    "        # a terminal state is reached) into a replay memory.\n",
    "        #Initialize replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed) ###\n",
    "        # Initialize time step (update every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps\n",
    "        self.t_step =(self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # if enough samples are availabe in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE: ###\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA) ###\n",
    "                \n",
    "    def act(self, state, eps=0):\n",
    "        '''\n",
    "        Choose action A from state S using policy pi <- epsilon-Greedt(q^hat (S,A,w))\n",
    "        Return actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection        \n",
    "        '''\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        # It is off-policy: it learns about the greedy policy a = argmax Q(s,a';θ),\n",
    "        # while following a behaviour distribution is often selected by an eps-greedy policy\n",
    "        # that follows the greey policy with probability 1-eps and selects a random action\n",
    "        # with probability eps.        \n",
    "        # Epsilon-greedy action selection\n",
    "        # \n",
    "        # with probability epsilon select a random action a_t\n",
    "        # otherwise select a_t = argmax_a Q (phi(s_t),a; θ)\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        \n",
    "    def learn(self, experiences, gamma): #----only use the local and target Q-networks to compute the loss before taking a step towards minimizing the loss\n",
    "        '''\n",
    "        Update value parameters using given batch of experience tuples\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \n",
    "        '''\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        #####DQN\n",
    "        ## Get max predicted Q values (for next states) from target model\n",
    "        #Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        #\n",
    "        ## Compute Q targets for current states\n",
    "        #Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        #\n",
    "        ## Get expected Q values from local model\n",
    "        #Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        ##### Double DQN\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            # fetch max action arguemnt to pass\n",
    "            Q_pred = self.qnetwork_local(next_states)\n",
    "            max_actions = torch.argmax(Q_pred, dim=1).long().unsqueeze(1)\n",
    "            # Q_targets over next statesfrom actions will be taken based on Q_pred's max_action\n",
    "            Q_next = self.qnetwork_target(next_states)\n",
    "        self.qnetwork_local.train()\n",
    "        Q_targets = rewards + (gamma * Q_next.gather(1, max_actions) * (1.0 - dones))\n",
    "        ## Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        \n",
    "        ###############\n",
    "        # apply loss fucntion\n",
    "        # calculate the loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # zero the parameter (weight) gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # backward pass to calculate the parameter gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the parameters\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        #################\n",
    "        #Update target network\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU) ###\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97d648e",
   "metadata": {},
   "source": [
    "# Watch an untrained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdd7e93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=8, action_size=4, seed=0)\n",
    "\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "agent.qnetwork_target.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dbd6b6",
   "metadata": {},
   "source": [
    "# Train the Agent with DQN\n",
    "Train Deep Q-Learning with\n",
    "* maximum number of training episodes: 2000\n",
    "* maximum number of timesteps per episode: 1000\n",
    "* starting value of epsilon for epsilon-greedy action selection: 1.0\n",
    "* minimum value of epsilon: 0.01\n",
    "* multiplacative factor (per episode for decreasing epsilon: 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91021fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\t Average Score: -384.38\n",
      "Episode 2\t Average Score: -331.23\n",
      "Episode 3\t Average Score: -238.31\n",
      "Episode 4\t Average Score: -237.00\n",
      "Episode 5\t Average Score: -239.07\n",
      "Episode 6\t Average Score: -214.21\n",
      "Episode 7\t Average Score: -200.59\n",
      "Episode 8\t Average Score: -176.54\n",
      "Episode 9\t Average Score: -166.35\n",
      "Episode 10\t Average Score: -155.12\n",
      "Episode 11\t Average Score: -150.85\n",
      "Episode 12\t Average Score: -141.18\n",
      "Episode 13\t Average Score: -149.22\n",
      "Episode 14\t Average Score: -151.48\n",
      "Episode 15\t Average Score: -147.77\n",
      "Episode 16\t Average Score: -141.44\n",
      "Episode 17\t Average Score: -135.93\n",
      "Episode 18\t Average Score: -139.23\n",
      "Episode 19\t Average Score: -139.11\n",
      "Episode 20\t Average Score: -137.97\n",
      "Episode 21\t Average Score: -146.22\n",
      "Episode 22\t Average Score: -146.80\n",
      "Episode 23\t Average Score: -144.90\n",
      "Episode 24\t Average Score: -142.67\n",
      "Episode 25\t Average Score: -141.11\n",
      "Episode 26\t Average Score: -142.86\n",
      "Episode 27\t Average Score: -141.81\n",
      "Episode 28\t Average Score: -140.06\n",
      "Episode 29\t Average Score: -137.38\n",
      "Episode 30\t Average Score: -137.21\n",
      "Episode 31\t Average Score: -137.79\n",
      "Episode 32\t Average Score: -136.23\n",
      "Episode 33\t Average Score: -134.82\n",
      "Episode 34\t Average Score: -135.99\n",
      "Episode 35\t Average Score: -133.56\n",
      "Episode 36\t Average Score: -135.78\n",
      "Episode 37\t Average Score: -135.37\n",
      "Episode 38\t Average Score: -133.79\n",
      "Episode 39\t Average Score: -132.51\n",
      "Episode 40\t Average Score: -131.94\n",
      "Episode 41\t Average Score: -134.17\n",
      "Episode 42\t Average Score: -135.22\n",
      "Episode 43\t Average Score: -134.53\n",
      "Episode 44\t Average Score: -134.85\n",
      "Episode 45\t Average Score: -133.06\n",
      "Episode 46\t Average Score: -131.72\n",
      "Episode 47\t Average Score: -129.94\n",
      "Episode 48\t Average Score: -129.93\n",
      "Episode 49\t Average Score: -127.64\n",
      "Episode 50\t Average Score: -131.02\n",
      "Episode 51\t Average Score: -130.24\n",
      "Episode 52\t Average Score: -128.44\n",
      "Episode 53\t Average Score: -128.37\n",
      "Episode 54\t Average Score: -129.58\n",
      "Episode 55\t Average Score: -128.52\n",
      "Episode 56\t Average Score: -127.90\n",
      "Episode 57\t Average Score: -127.00\n",
      "Episode 58\t Average Score: -126.37\n",
      "Episode 59\t Average Score: -126.91\n",
      "Episode 60\t Average Score: -125.51\n",
      "Episode 61\t Average Score: -126.35\n",
      "Episode 62\t Average Score: -125.80\n",
      "Episode 63\t Average Score: -124.18\n",
      "Episode 64\t Average Score: -126.24\n",
      "Episode 65\t Average Score: -124.61\n",
      "Episode 66\t Average Score: -124.34\n",
      "Episode 67\t Average Score: -123.90\n",
      "Episode 68\t Average Score: -123.97\n",
      "Episode 69\t Average Score: -123.09\n",
      "Episode 70\t Average Score: -125.51\n",
      "Episode 71\t Average Score: -123.51\n",
      "Episode 72\t Average Score: -122.72\n",
      "Episode 73\t Average Score: -122.26\n",
      "Episode 74\t Average Score: -122.63\n",
      "Episode 75\t Average Score: -122.30\n",
      "Episode 76\t Average Score: -122.21\n",
      "Episode 77\t Average Score: -122.39\n",
      "Episode 78\t Average Score: -122.03\n",
      "Episode 79\t Average Score: -121.16\n",
      "Episode 80\t Average Score: -120.45\n",
      "Episode 81\t Average Score: -119.16\n",
      "Episode 82\t Average Score: -119.21\n",
      "Episode 83\t Average Score: -118.48\n",
      "Episode 84\t Average Score: -118.00\n",
      "Episode 85\t Average Score: -117.15\n",
      "Episode 86\t Average Score: -115.86\n",
      "Episode 87\t Average Score: -115.81\n",
      "Episode 88\t Average Score: -114.69\n",
      "Episode 89\t Average Score: -113.93\n",
      "Episode 90\t Average Score: -114.12\n",
      "Episode 91\t Average Score: -113.87\n",
      "Episode 92\t Average Score: -113.48\n",
      "Episode 93\t Average Score: -113.03\n",
      "Episode 94\t Average Score: -112.82\n",
      "Episode 95\t Average Score: -111.97\n",
      "Episode 96\t Average Score: -111.30\n",
      "Episode 97\t Average Score: -110.92\n",
      "Episode 98\t Average Score: -110.43\n",
      "Episode 99\t Average Score: -109.68\n",
      "Episode 100\t Average Score: -109.11\n",
      "Episode 100\t Average Score:-109.11\n",
      "Episode 101\t Average Score: -107.25\n",
      "Episode 102\t Average Score: -104.38\n",
      "Episode 103\t Average Score: -104.56\n",
      "Episode 104\t Average Score: -103.00\n",
      "Episode 105\t Average Score: -100.99\n",
      "Episode 106\t Average Score: -101.29\n",
      "Episode 107\t Average Score: -100.94\n",
      "Episode 108\t Average Score: -102.69\n",
      "Episode 109\t Average Score: -102.24\n",
      "Episode 110\t Average Score: -102.39\n",
      "Episode 111\t Average Score: -101.02\n",
      "Episode 112\t Average Score: -100.75\n",
      "Episode 113\t Average Score: -98.13\n",
      "Episode 114\t Average Score: -97.61\n",
      "Episode 115\t Average Score: -96.68\n",
      "Episode 116\t Average Score: -97.01\n",
      "Episode 117\t Average Score: -97.77\n",
      "Episode 118\t Average Score: -96.11\n",
      "Episode 119\t Average Score: -94.98\n",
      "Episode 120\t Average Score: -94.00\n",
      "Episode 121\t Average Score: -91.57\n",
      "Episode 122\t Average Score: -89.88\n",
      "Episode 123\t Average Score: -89.50\n",
      "Episode 124\t Average Score: -88.73\n",
      "Episode 125\t Average Score: -87.53\n",
      "Episode 126\t Average Score: -84.96\n",
      "Episode 127\t Average Score: -84.70\n",
      "Episode 128\t Average Score: -83.80\n",
      "Episode 129\t Average Score: -82.89\n",
      "Episode 130\t Average Score: -83.08\n",
      "Episode 131\t Average Score: -82.36\n",
      "Episode 132\t Average Score: -81.97\n",
      "Episode 133\t Average Score: -81.64\n",
      "Episode 134\t Average Score: -80.55\n",
      "Episode 135\t Average Score: -80.15\n",
      "Episode 136\t Average Score: -78.51\n",
      "Episode 137\t Average Score: -77.46\n",
      "Episode 138\t Average Score: -76.43\n",
      "Episode 139\t Average Score: -75.92\n",
      "Episode 140\t Average Score: -75.44\n",
      "Episode 141\t Average Score: -73.87\n",
      "Episode 142\t Average Score: -73.35\n",
      "Episode 143\t Average Score: -73.22\n",
      "Episode 144\t Average Score: -72.16\n",
      "Episode 145\t Average Score: -73.58\n",
      "Episode 146\t Average Score: -73.09\n",
      "Episode 147\t Average Score: -72.75\n",
      "Episode 148\t Average Score: -72.10\n",
      "Episode 149\t Average Score: -71.98\n",
      "Episode 150\t Average Score: -69.27\n",
      "Episode 151\t Average Score: -69.18\n",
      "Episode 152\t Average Score: -67.60\n",
      "Episode 153\t Average Score: -66.58\n",
      "Episode 154\t Average Score: -64.72\n",
      "Episode 155\t Average Score: -63.27\n",
      "Episode 156\t Average Score: -62.40\n",
      "Episode 157\t Average Score: -62.58\n",
      "Episode 158\t Average Score: -61.63\n",
      "Episode 159\t Average Score: -60.00\n",
      "Episode 160\t Average Score: -60.27\n",
      "Episode 161\t Average Score: -58.54\n",
      "Episode 162\t Average Score: -57.91\n",
      "Episode 163\t Average Score: -58.55\n",
      "Episode 164\t Average Score: -56.61\n",
      "Episode 165\t Average Score: -56.20\n",
      "Episode 166\t Average Score: -55.17\n",
      "Episode 167\t Average Score: -54.64\n",
      "Episode 168\t Average Score: -53.69\n",
      "Episode 169\t Average Score: -53.31\n",
      "Episode 170\t Average Score: -49.60\n",
      "Episode 171\t Average Score: -49.23\n",
      "Episode 172\t Average Score: -48.84\n",
      "Episode 173\t Average Score: -47.82\n",
      "Episode 174\t Average Score: -45.97\n",
      "Episode 175\t Average Score: -45.45\n",
      "Episode 176\t Average Score: -44.87\n",
      "Episode 177\t Average Score: -42.57\n",
      "Episode 178\t Average Score: -41.71\n",
      "Episode 179\t Average Score: -41.60\n",
      "Episode 180\t Average Score: -40.41\n",
      "Episode 181\t Average Score: -40.18\n",
      "Episode 182\t Average Score: -38.11\n",
      "Episode 183\t Average Score: -38.31\n",
      "Episode 184\t Average Score: -36.68\n",
      "Episode 185\t Average Score: -35.88\n",
      "Episode 186\t Average Score: -35.10\n",
      "Episode 187\t Average Score: -33.89\n",
      "Episode 188\t Average Score: -33.90\n",
      "Episode 189\t Average Score: -34.13\n",
      "Episode 190\t Average Score: -31.67\n",
      "Episode 191\t Average Score: -30.75\n",
      "Episode 192\t Average Score: -29.89\n",
      "Episode 193\t Average Score: -29.00\n",
      "Episode 194\t Average Score: -27.94\n",
      "Episode 195\t Average Score: -27.44\n",
      "Episode 196\t Average Score: -25.83\n",
      "Episode 197\t Average Score: -23.67\n",
      "Episode 198\t Average Score: -22.15\n",
      "Episode 199\t Average Score: -21.90\n",
      "Episode 200\t Average Score: -19.86\n",
      "Episode 200\t Average Score:-19.86\n",
      "Episode 201\t Average Score: -18.23\n",
      "Episode 202\t Average Score: -18.05\n",
      "Episode 203\t Average Score: -17.06\n",
      "Episode 204\t Average Score: -16.78\n",
      "Episode 205\t Average Score: -15.83\n",
      "Episode 206\t Average Score: -14.32\n",
      "Episode 207\t Average Score: -12.77\n",
      "Episode 208\t Average Score: -10.69\n",
      "Episode 209\t Average Score: -10.28\n",
      "Episode 210\t Average Score: -9.08\n",
      "Episode 211\t Average Score: -8.92\n",
      "Episode 212\t Average Score: -8.92\n",
      "Episode 213\t Average Score: -9.38\n",
      "Episode 214\t Average Score: -8.71\n",
      "Episode 215\t Average Score: -8.58\n",
      "Episode 216\t Average Score: -7.85\n",
      "Episode 217\t Average Score: -6.74\n",
      "Episode 218\t Average Score: -6.42\n",
      "Episode 219\t Average Score: -6.56\n",
      "Episode 220\t Average Score: -5.86\n",
      "Episode 221\t Average Score: -5.03\n",
      "Episode 222\t Average Score: -5.11\n",
      "Episode 223\t Average Score: -4.31\n",
      "Episode 224\t Average Score: -4.46\n",
      "Episode 225\t Average Score: -4.32\n",
      "Episode 226\t Average Score: -5.22\n",
      "Episode 227\t Average Score: -4.24\n",
      "Episode 228\t Average Score: -4.53\n",
      "Episode 229\t Average Score: -4.09\n",
      "Episode 230\t Average Score: -2.20\n",
      "Episode 231\t Average Score: -1.39\n",
      "Episode 232\t Average Score: -0.96\n",
      "Episode 233\t Average Score: -0.33\n",
      "Episode 234\t Average Score: 0.83\n",
      "Episode 235\t Average Score: 0.49\n",
      "Episode 236\t Average Score: 0.91\n",
      "Episode 237\t Average Score: 1.52\n",
      "Episode 238\t Average Score: 1.00\n",
      "Episode 239\t Average Score: 1.41\n",
      "Episode 240\t Average Score: 2.14\n",
      "Episode 241\t Average Score: 2.67\n",
      "Episode 242\t Average Score: 4.04\n",
      "Episode 243\t Average Score: 4.91\n",
      "Episode 244\t Average Score: 5.69\n",
      "Episode 245\t Average Score: 7.33\n",
      "Episode 246\t Average Score: 7.81\n",
      "Episode 247\t Average Score: 8.31\n",
      "Episode 248\t Average Score: 8.94\n",
      "Episode 249\t Average Score: 9.00\n",
      "Episode 250\t Average Score: 8.77\n",
      "Episode 251\t Average Score: 9.45\n",
      "Episode 252\t Average Score: 8.10\n",
      "Episode 253\t Average Score: 8.61\n",
      "Episode 254\t Average Score: 9.42\n",
      "Episode 255\t Average Score: 8.71\n",
      "Episode 256\t Average Score: 8.76\n",
      "Episode 257\t Average Score: 9.66\n",
      "Episode 258\t Average Score: 9.52\n",
      "Episode 259\t Average Score: 9.51\n",
      "Episode 260\t Average Score: 10.23\n",
      "Episode 261\t Average Score: 10.34\n",
      "Episode 262\t Average Score: 9.67\n",
      "Episode 263\t Average Score: 9.98\n",
      "Episode 264\t Average Score: 10.76\n",
      "Episode 265\t Average Score: 9.96\n",
      "Episode 266\t Average Score: 10.27\n",
      "Episode 267\t Average Score: 10.35\n",
      "Episode 268\t Average Score: 11.15\n",
      "Episode 269\t Average Score: 11.14\n",
      "Episode 270\t Average Score: 9.86\n",
      "Episode 271\t Average Score: 9.17\n",
      "Episode 272\t Average Score: 9.13\n",
      "Episode 273\t Average Score: 9.28\n",
      "Episode 274\t Average Score: 9.00\n",
      "Episode 275\t Average Score: 9.87\n",
      "Episode 276\t Average Score: 10.04\n",
      "Episode 277\t Average Score: 8.65\n",
      "Episode 278\t Average Score: 8.43\n",
      "Episode 279\t Average Score: 8.92\n",
      "Episode 280\t Average Score: 7.43\n",
      "Episode 281\t Average Score: 7.06\n",
      "Episode 282\t Average Score: 6.11\n",
      "Episode 283\t Average Score: 7.32\n",
      "Episode 284\t Average Score: 6.14\n",
      "Episode 285\t Average Score: 5.35\n",
      "Episode 286\t Average Score: 4.85\n",
      "Episode 287\t Average Score: 4.50\n",
      "Episode 288\t Average Score: 7.81\n",
      "Episode 289\t Average Score: 8.70\n",
      "Episode 290\t Average Score: 8.81\n",
      "Episode 291\t Average Score: 8.33\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m#double deuling dqn\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mdqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# plot the scores\u001b[39;00m\n\u001b[0;32m     69\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
      "Cell \u001b[1;32mIn[16], line 35\u001b[0m, in \u001b[0;36mdqn\u001b[1;34m(n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Set constrain maximum number of time step per episode\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_t):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# agent select an action\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# agent performs the selected action\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Cell \u001b[1;32mIn[5], line 93\u001b[0m, in \u001b[0;36mAgent.act\u001b[1;34m(self, state, eps)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# It is off-policy: it learns about the greedy policy a = argmax Q(s,a';θ),\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# while following a behaviour distribution is often selected by an eps-greedy policy\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# that follows the greey policy with probability 1-eps and selects a random action\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# with probability epsilon select a random action a_t\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# otherwise select a_t = argmax_a Q (phi(s_t),a; θ)\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m>\u001b[39m eps:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m random\u001b[38;5;241m.\u001b[39mchoice(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_size))\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32md:\\install\\anaconda\\envs\\vk\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1242\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\install\\anaconda\\envs\\vk\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#double dueling dqn\n",
    "def dqn(n_episodes = 2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"\n",
    "    Train the Agent with Deep Q-Learning\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    # Initialize collecting scores from each episode\n",
    "    scores = []\n",
    "    # Initialize collecting maxlen(100) scores\n",
    "    scores_window = deque(maxlen=100)\n",
    "    # initialize starting value of epsilon\n",
    "    eps = eps_start\n",
    "    \n",
    "    # for each episode----------------\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # begin the episode\n",
    "        state = env.reset()\n",
    "        # initialize the sampled score(reward)\n",
    "        score = 0\n",
    "        \n",
    "        # Set constrain maximum number of time step per episode\n",
    "        for t in range(max_t):\n",
    "            # agent select an action\n",
    "            action = agent.act(state, eps)\n",
    "            # agent performs the selected action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # agent performs internal updates based on sampled experience\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            # update the sampled reward\n",
    "            score += reward\n",
    "            # update the state (s <- s') to next time step\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        #Save most recent score\n",
    "        scores_window.append(score)\n",
    "        #save most recent score\n",
    "        scores.append(score)\n",
    "        #Decrease epsilon\n",
    "        eps = max(eps_end, eps_decay*eps)\n",
    "        \n",
    "        # monitor progress\n",
    "        print('\\rEpisode {}\\t Average Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        # get average reward from last 100 episodes\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\t Average Score:{:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=300.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100,\n",
    "                                                                                         np.mean(scores_window)))\n",
    "            # save model\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "#double deuling dqn\n",
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6b25cb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'display'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m img\u001b[38;5;241m.\u001b[39mset_data(env\u001b[38;5;241m.\u001b[39mrender(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m'\u001b[39m)) \n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m(plt\u001b[38;5;241m.\u001b[39mgcf())\n\u001b[0;32m     12\u001b[0m display\u001b[38;5;241m.\u001b[39mclear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'display'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the weights from file\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "for i in range(7):\n",
    "    state = env.reset()\n",
    "    img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    for j in range(500):\n",
    "        action = agent.act(state)\n",
    "        img.set_data(env.render(mode='rgb_array')) \n",
    "        plt.axis('off')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ceb894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0bd6de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b601e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
