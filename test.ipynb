{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib as plt\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim) -> None:\n",
    "        super(DQN,self).__init__( )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_dim,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple\n",
    "class RelpayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        \n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.example =  namedtuple(\"example\",field_names=[\"state\",\"action\",\"reward\",\"next_state\",\"done\"])\n",
    "    \n",
    "    def push(self,state,action,reward,next_state,done):\n",
    "        \n",
    "        example = self.example(state,action,reward,next_state,done)\n",
    "        self.memory.append(example)\n",
    "    def sample(self):\n",
    "        \n",
    "        experiences = random.sample(self.memory,k=self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device) # gpu\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states,actions,rewards,next_states,dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 5e-4\n",
    "buffer_size = int(1e5)\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "up = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.in_dim = 8 \n",
    "        self.out_dim = 4\n",
    "        self.model = DQN(self.in_dim,self.out_dim).to(device)\n",
    "        self.targetmodel = DQN(self.in_dim,self.out_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(),lr=lr)\n",
    "        self.memory = RelpayBuffer(buffer_size,batch_size)\n",
    "        self.t_step = 0\n",
    "        self.scheduler = StepLR(self.optimizer, step_size=80, gamma=1)\n",
    "    def step(self, state, action, reward, next_state,done):\n",
    "        self.memory.push(state,action,reward,next_state,done)\n",
    "        self.t_step = (self.t_step + 1) % up\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) < batch_size:\n",
    "                return\n",
    "            else:\n",
    "                train_set = self.memory.sample()\n",
    "                self.train(train_set)\n",
    "\n",
    "    def train(self,train_set):\n",
    "        tau = 1e-3\n",
    "        state, action, reward, next_state, done = train_set\n",
    "         \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            q_pred = self.model(next_state)\n",
    "            max_action = torch.argmax(q_pred,dim=1).long().unsqueeze(1)\n",
    "            q_next = self.targetmodel(next_state)\n",
    "        self.model.train()\n",
    "        q_targets = reward + (gamma * q_next.gather(1,max_action) * (1.0 - done))\n",
    "        q_expect = self.model(state).gather(1,action)\n",
    "        loss = F.mse_loss(q_expect,q_targets)\n",
    "        self.scheduler.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step() \n",
    "        self.update(self.model,self.targetmodel,tau)\n",
    "\n",
    "    def choose_action(self,state,epsilon):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            action_value = self.model(state)\n",
    "        self.model.train()\n",
    "\n",
    "        if random.random() > epsilon:\n",
    "            return np.argmax(action_value.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(4))\n",
    "    \n",
    "    def update(self, local_model, target_model, tau):\n",
    " \n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "              \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent()\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch = 2000,maxt = 1000,eps_start = 1, eps_end = 0.01, eps_decay = 0.995):\n",
    "\n",
    "    rewards = []\n",
    "    avg_rewards = []\n",
    "    epochs = [] \n",
    "    epsilon = eps_start\n",
    "    \n",
    "    for i in range(1,epoch +1):\n",
    "        epochs.append(i)\n",
    "        state = env.reset()\n",
    "\n",
    "        score = 0\n",
    "        \n",
    "        for j in range(maxt):\n",
    "            action = agent.choose_action(state,epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action ,reward, next_state, done)\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(score)\n",
    "        avg_rewards.append(np.mean(rewards[-100:]))\n",
    "        epsilon = max(eps_end,epsilon* eps_decay)\n",
    "\n",
    "        # if score>200 and np.mean(rewards[-100:])>200:\n",
    "        #     torch.save(agent.model, f'good_model{i}.pth')\n",
    "            \n",
    "        if (i + 1) % 1000 ==0:\n",
    "            torch.save(agent.model, f'good_model{i}.pth')\n",
    "            plt.figure(figsize=(24,6))\n",
    "            plt.plot(epochs,rewards,label=\"reward\")\n",
    "            plt.plot(epochs,avg_rewards,label = \"avg_reward\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.legend()\n",
    "            plt.savefig(f\"model{i}reward.png\", dpi = 400)\n",
    "            plt.close()\n",
    "           \n",
    "\n",
    "        print(f\"epoch{i} reward:{score} avg_reward:{np.mean(rewards[-100:])} lr:{agent.optimizer.param_groups[0]['lr']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1 reward:-285.76458040849184 avg_reward:-285.76458040849184 lr:0.0005\n",
      "epoch2 reward:-132.00717267999326 avg_reward:-208.88587654424254 lr:0.0005\n",
      "epoch3 reward:-284.127703985861 avg_reward:-233.9664856914487 lr:0.0005\n",
      "epoch4 reward:-402.4293587423192 avg_reward:-276.0822039541663 lr:0.0005\n",
      "epoch5 reward:-167.35131745987906 avg_reward:-254.33602665530884 lr:0.0005\n",
      "epoch6 reward:-368.1569736106394 avg_reward:-273.30618448119725 lr:0.0005\n",
      "epoch7 reward:-148.38509048350087 avg_reward:-255.46031391009777 lr:0.0005\n",
      "epoch8 reward:-140.76788096228205 avg_reward:-241.1237597916208 lr:0.0005\n",
      "epoch9 reward:-433.9893860713821 avg_reward:-262.5532738227054 lr:0.0005\n",
      "epoch10 reward:-123.21836976392524 avg_reward:-248.61978341682737 lr:0.0005\n",
      "epoch11 reward:-73.88078102774965 avg_reward:-232.73441956327486 lr:0.0005\n",
      "epoch12 reward:-216.62685233885946 avg_reward:-231.39212229457357 lr:0.0005\n",
      "epoch13 reward:-283.6356429737348 avg_reward:-235.41085465450902 lr:0.0005\n",
      "epoch14 reward:-107.29732679513805 avg_reward:-226.25988837883966 lr:0.0005\n",
      "epoch15 reward:-259.8257531044127 avg_reward:-228.49761269387787 lr:0.0005\n",
      "epoch16 reward:-107.1259871295703 avg_reward:-220.91188609610867 lr:0.0005\n",
      "epoch17 reward:-215.85947197781212 avg_reward:-220.61468526562064 lr:0.0005\n",
      "epoch18 reward:-136.8651280244618 avg_reward:-215.96193208555624 lr:0.0005\n",
      "epoch19 reward:-167.51895094982143 avg_reward:-213.41230149946495 lr:0.0005\n",
      "epoch20 reward:-123.15118212626618 avg_reward:-208.899245530805 lr:0.0005\n",
      "epoch21 reward:-127.12107252327777 avg_reward:-205.00504681616084 lr:0.0005\n",
      "epoch22 reward:-102.59779842612696 avg_reward:-200.35017188934114 lr:0.0005\n",
      "epoch23 reward:-198.35887659780207 avg_reward:-200.26359383318726 lr:0.0005\n",
      "epoch24 reward:-102.25451614041432 avg_reward:-196.17988226265507 lr:0.0005\n",
      "epoch25 reward:-109.23178623859974 avg_reward:-192.70195842169286 lr:0.0005\n",
      "epoch26 reward:-87.16368368585674 avg_reward:-188.6427940087761 lr:0.0005\n",
      "epoch27 reward:-145.50842054787157 avg_reward:-187.0452246213352 lr:0.0005\n",
      "epoch28 reward:-98.12372609819855 avg_reward:-183.86945681693746 lr:0.0005\n",
      "epoch29 reward:-106.1405578181181 avg_reward:-181.18914995490923 lr:0.0005\n",
      "epoch30 reward:-47.77266344772001 avg_reward:-176.74193373800293 lr:0.0005\n",
      "epoch31 reward:-404.74970484397335 avg_reward:-184.09702312851812 lr:0.0005\n",
      "epoch32 reward:-250.40785422996575 avg_reward:-186.1692366004383 lr:0.0005\n",
      "epoch33 reward:-253.2985986218591 avg_reward:-188.2034596919965 lr:0.0005\n",
      "epoch34 reward:-7.784615943117771 avg_reward:-182.89702311114712 lr:0.0005\n",
      "epoch35 reward:-378.0942013497372 avg_reward:-188.4740853465354 lr:0.0005\n",
      "epoch36 reward:-127.84220809185072 avg_reward:-186.78986653390527 lr:0.0005\n",
      "epoch37 reward:-104.01221844071814 avg_reward:-184.55263280165698 lr:0.0005\n",
      "epoch38 reward:-212.43735293503102 avg_reward:-185.28644122621947 lr:0.0005\n",
      "epoch39 reward:-276.321416123119 avg_reward:-187.620671351781 lr:0.0005\n",
      "epoch40 reward:-287.88495304647006 avg_reward:-190.1272783941482 lr:0.0005\n",
      "epoch41 reward:-141.26220061638946 avg_reward:-188.935447228837 lr:0.0005\n",
      "epoch42 reward:-94.19476753019231 avg_reward:-186.67971675982167 lr:0.0005\n",
      "epoch43 reward:-174.3714705479201 avg_reward:-186.39347847582394 lr:0.0005\n",
      "epoch44 reward:-300.3920173926337 avg_reward:-188.98435436029692 lr:0.0005\n",
      "epoch45 reward:-100.31800998500819 avg_reward:-187.01399115195716 lr:0.0005\n",
      "epoch46 reward:-223.20063412261288 avg_reward:-187.80065730349318 lr:0.0005\n",
      "epoch47 reward:-148.67598702298903 avg_reward:-186.96821751029094 lr:0.0005\n",
      "epoch48 reward:-60.27232668160623 avg_reward:-184.32871978469333 lr:0.0005\n",
      "epoch49 reward:-93.49693365118011 avg_reward:-182.47500986360123 lr:0.0005\n",
      "epoch50 reward:-258.3681843465711 avg_reward:-183.99287335326062 lr:0.0005\n",
      "epoch51 reward:-98.88191551199165 avg_reward:-182.32403104264748 lr:0.0005\n",
      "epoch52 reward:-53.411102038782296 avg_reward:-179.84493625411162 lr:0.0005\n",
      "epoch53 reward:-140.13367384093758 avg_reward:-179.09566715197627 lr:0.0005\n",
      "epoch54 reward:-15.592003080197898 avg_reward:-176.06782152101738 lr:0.0005\n",
      "epoch55 reward:-80.31146555996997 avg_reward:-174.32679686718015 lr:0.0005\n",
      "epoch56 reward:-316.9813789010167 avg_reward:-176.87420011778445 lr:0.0005\n",
      "epoch57 reward:-225.52981373198378 avg_reward:-177.72780737417392 lr:0.0005\n",
      "epoch58 reward:-256.30313694880545 avg_reward:-179.0825544358055 lr:0.0005\n",
      "epoch59 reward:-87.87030612523216 avg_reward:-177.53658412545678 lr:0.0005\n",
      "epoch60 reward:-133.4787867753924 avg_reward:-176.80228750295572 lr:0.0005\n",
      "epoch61 reward:-372.8648510028481 avg_reward:-180.01642788819987 lr:0.0005\n",
      "epoch62 reward:-54.78796090503273 avg_reward:-177.9966139046004 lr:0.0005\n",
      "epoch63 reward:-123.8774402127726 avg_reward:-177.1375794015555 lr:0.0005\n",
      "epoch64 reward:-145.28768900830994 avg_reward:-176.639924864161 lr:0.0005\n",
      "epoch65 reward:-198.948005720151 avg_reward:-176.9831261080993 lr:0.0005\n",
      "epoch66 reward:-192.7310113157351 avg_reward:-177.2217304294271 lr:0.0005\n",
      "epoch67 reward:-325.1030349629199 avg_reward:-179.42891407918074 lr:0.0005\n",
      "epoch68 reward:-164.06096683859323 avg_reward:-179.20291485505444 lr:0.0005\n",
      "epoch69 reward:-184.270937600868 avg_reward:-179.27636446006625 lr:0.0005\n",
      "epoch70 reward:-143.98894570706307 avg_reward:-178.77225847788046 lr:0.0005\n",
      "epoch71 reward:-218.59452176020605 avg_reward:-179.33313542551886 lr:0.0005\n",
      "epoch72 reward:-225.48713840307425 avg_reward:-179.97416324465158 lr:0.0005\n",
      "epoch73 reward:-215.54405787640803 avg_reward:-180.4614220752236 lr:0.0005\n",
      "epoch74 reward:-89.30183393663248 avg_reward:-179.2295357490264 lr:0.0005\n",
      "epoch75 reward:-64.31752069237841 avg_reward:-177.6973755482711 lr:0.0005\n",
      "epoch76 reward:-127.15511807498815 avg_reward:-177.0323458446753 lr:0.0005\n",
      "epoch77 reward:-160.7251063405157 avg_reward:-176.82056351345244 lr:0.0005\n",
      "epoch78 reward:-110.12240364541692 avg_reward:-175.96545889975968 lr:0.0005\n",
      "epoch79 reward:-193.74900398761497 avg_reward:-176.19056706542875 lr:0.0005\n",
      "epoch80 reward:-244.5215083862948 avg_reward:-177.04470383193956 lr:0.0005\n",
      "epoch81 reward:-191.3372111314756 avg_reward:-177.22115453934123 lr:0.0005\n",
      "epoch82 reward:-122.65727563003428 avg_reward:-176.55574138191065 lr:0.0005\n",
      "epoch83 reward:-112.29659393599425 avg_reward:-175.7815347861767 lr:0.0005\n",
      "epoch84 reward:-74.10405194742741 avg_reward:-174.5710885619059 lr:0.0005\n",
      "epoch85 reward:-75.49320878711137 avg_reward:-173.40546644690832 lr:0.0005\n",
      "epoch86 reward:-358.50065782360775 avg_reward:-175.55773611407923 lr:0.0005\n",
      "epoch87 reward:-205.02731766959488 avg_reward:-175.89646693655644 lr:0.0005\n",
      "epoch88 reward:-174.38867723031044 avg_reward:-175.87933296262182 lr:0.0005\n",
      "epoch89 reward:-260.97982237614315 avg_reward:-176.83551823693105 lr:0.0005\n",
      "epoch90 reward:-264.5720514553442 avg_reward:-177.81036860602453 lr:0.0005\n",
      "epoch91 reward:-39.97173834350086 avg_reward:-176.29565838335944 lr:0.0005\n",
      "epoch92 reward:-177.82786631579728 avg_reward:-176.31231281740767 lr:0.0005\n",
      "epoch93 reward:-65.03894525499157 avg_reward:-175.11582499415587 lr:0.0005\n",
      "epoch94 reward:-43.90324942162837 avg_reward:-173.71994653061836 lr:0.0005\n",
      "epoch95 reward:-186.5368090594422 avg_reward:-173.85486087302704 lr:0.0005\n",
      "epoch96 reward:-155.48056232633405 avg_reward:-173.66346192983232 lr:0.0005\n",
      "epoch97 reward:-358.5848172553152 avg_reward:-175.5698676548373 lr:0.0005\n",
      "epoch98 reward:-126.97126216522186 avg_reward:-175.07396351718816 lr:0.0005\n",
      "epoch99 reward:-93.60909372591962 avg_reward:-174.2510860445491 lr:0.0005\n",
      "epoch100 reward:-33.93455545842947 avg_reward:-172.8479207386879 lr:0.0005\n",
      "epoch101 reward:-183.65313379916662 avg_reward:-171.8268062725946 lr:0.0005\n",
      "epoch102 reward:-83.80345773185834 avg_reward:-171.34476912311328 lr:0.0005\n",
      "epoch103 reward:-296.3428909128459 avg_reward:-171.46692099238308 lr:0.0005\n",
      "epoch104 reward:-250.95042414756585 avg_reward:-169.9521316464356 lr:0.0005\n",
      "epoch105 reward:-16.056911114249118 avg_reward:-168.4391875829793 lr:0.0005\n",
      "epoch106 reward:-6.169743160467263 avg_reward:-164.81931527847755 lr:0.0005\n",
      "epoch107 reward:-147.01936580736412 avg_reward:-164.80565803171623 lr:0.0005\n",
      "epoch108 reward:-98.20822117125046 avg_reward:-164.3800614338059 lr:0.0005\n",
      "epoch109 reward:-121.92984121502298 avg_reward:-161.25946598524231 lr:0.0005\n",
      "epoch110 reward:-117.27066048502135 avg_reward:-161.19998889245326 lr:0.0005\n",
      "epoch111 reward:-113.74228864815174 avg_reward:-161.59860396865727 lr:0.0005\n",
      "epoch112 reward:-95.47978911051942 avg_reward:-160.38713333637386 lr:0.0005\n",
      "epoch113 reward:-62.307443263437534 avg_reward:-158.17385133927087 lr:0.0005\n",
      "epoch114 reward:-131.93322633164357 avg_reward:-158.42021033463593 lr:0.0005\n",
      "epoch115 reward:-94.28396839739996 avg_reward:-156.76479248756584 lr:0.0005\n",
      "epoch116 reward:-42.48978754063526 avg_reward:-156.11843049167646 lr:0.0005\n",
      "epoch117 reward:-160.0548285288816 avg_reward:-155.56038405718718 lr:0.0005\n",
      "epoch118 reward:-68.12590620646598 avg_reward:-154.87299183900723 lr:0.0005\n",
      "epoch119 reward:-93.19909821385592 avg_reward:-154.12979331164757 lr:0.0005\n",
      "epoch120 reward:-260.20516656097584 avg_reward:-155.5003331559947 lr:0.0005\n",
      "epoch121 reward:-162.6737747942767 avg_reward:-155.8558601787047 lr:0.0005\n",
      "epoch122 reward:34.08025955856556 avg_reward:-154.48907959885776 lr:0.0005\n",
      "epoch123 reward:-152.21268582942122 avg_reward:-154.02761769117396 lr:0.0005\n",
      "epoch124 reward:-19.942716364695656 avg_reward:-153.20449969341675 lr:0.0005\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
