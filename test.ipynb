{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib as plt\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim) -> None:\n",
    "        super(DQN,self).__init__( )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_dim,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple\n",
    "class RelpayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        \n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.example =  namedtuple(\"example\",field_names=[\"state\",\"action\",\"reward\",\"next_state\",\"done\"])\n",
    "    \n",
    "    def push(self,state,action,reward,next_state,done):\n",
    "        \n",
    "        example = self.example(state,action,reward,next_state,done)\n",
    "        self.memory.append(example)\n",
    "    def sample(self):\n",
    "        \n",
    "        experiences = random.sample(self.memory,k=self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device) # gpu\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states,actions,rewards,next_states,dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 5e-4\n",
    "buffer_size = int(1e5)\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "up = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.in_dim = 8 \n",
    "        self.out_dim = 4\n",
    "        self.model = DQN(self.in_dim,self.out_dim)\n",
    "        self.targetmodel = DQN(self.in_dim,self.out_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(),lr=lr)\n",
    "        self.memory = RelpayBuffer(buffer_size,batch_size)\n",
    "        self.t_step = 0\n",
    "    def step(self, state, action, reward, next_state,done):\n",
    "        self.memory.push(state,action,reward,next_state,done)\n",
    "        self.t_step = (self.t_step + 1) % up\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) < batch_size:\n",
    "                return\n",
    "            else:\n",
    "                train_set = self.memory.sample()\n",
    "                self.train(train_set)\n",
    "\n",
    "    def train(self,train_set):\n",
    "        tau = 1e-3\n",
    "        state, action, reward, next_state, done = train_set\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            q_pred = self.model(next_state)\n",
    "            max_action = torch.argmax(q_pred,dim=1).long().unsqueeze(1)\n",
    "            q_next = self.targetmodel(next_state)\n",
    "        self.model.train()\n",
    "        q_targets = reward + (gamma * q_next.gather(1,max_action) * (1.0 - done))\n",
    "        q_expect = self.model(state).gather(1,action)\n",
    "        loss = F.mse_loss(q_expect,q_targets)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step() \n",
    "        self.update(self.model,self.targetmodel,tau)\n",
    "\n",
    "    def choose_action(self,state,epsilon):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            action_value = self.model(state)\n",
    "        self.model.train()\n",
    "\n",
    "        if random.random() > epsilon:\n",
    "            return np.argmax(action_value.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(4))\n",
    "    \n",
    "    def update(self, local_model, target_model, tau):\n",
    " \n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "              \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent()\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch = 2000,maxt = 1000,eps_start = 1, eps_end = 0.25, eps_decay = 0.995):\n",
    "\n",
    "    rewards = []\n",
    "    avg_rewards = []\n",
    "\n",
    "    epsilon = eps_start\n",
    "    \n",
    "    for i in range(1,epoch +1):\n",
    "        \n",
    "        state = env.reset()\n",
    "\n",
    "        score = 0\n",
    "        \n",
    "        for j in range(maxt):\n",
    "            action = agent.choose_action(state,epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action ,reward, next_state, done)\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(score)\n",
    "        avg_rewards.append(np.mean(rewards[-100:]))\n",
    "        epsilon = max(eps_end,epsilon* eps_decay)\n",
    "\n",
    "        print(f\"epoch{i} reward:{score} avg_reward:{np.mean(rewards[-100:])}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1 reward:-183.30163653802373 avg_reward:-183.30163653802373\n",
      "epoch2 reward:-273.07648876434996 avg_reward:-228.18906265118684\n",
      "epoch3 reward:-199.6962797203148 avg_reward:-218.6914683408962\n",
      "epoch4 reward:-416.0307169780808 avg_reward:-268.02628050019234\n",
      "epoch5 reward:-156.51534742458296 avg_reward:-245.7240938850705\n",
      "epoch6 reward:-261.89975539706677 avg_reward:-248.42003747040317\n",
      "epoch7 reward:-103.71063867228264 avg_reward:-227.7472662135288\n",
      "epoch8 reward:-459.59722937714855 avg_reward:-256.7285116089813\n",
      "epoch9 reward:-121.6721916374494 avg_reward:-241.72225383436665\n",
      "epoch10 reward:-340.9586304442572 avg_reward:-251.64589149535573\n",
      "epoch11 reward:-281.0030713116256 avg_reward:-254.31472602410756\n",
      "epoch12 reward:-113.61653997331067 avg_reward:-242.58987718654114\n",
      "epoch13 reward:-237.67400155466623 avg_reward:-242.21173290716615\n",
      "epoch14 reward:-275.5325869359283 avg_reward:-244.5917939092206\n",
      "epoch15 reward:-185.2022657868107 avg_reward:-240.63249203439327\n",
      "epoch16 reward:-301.5756723276805 avg_reward:-244.4414408027237\n",
      "epoch17 reward:-127.47554724503345 avg_reward:-237.56109412285954\n",
      "epoch18 reward:-105.61134973514216 avg_reward:-230.23055276798635\n",
      "epoch19 reward:-215.29101926477676 avg_reward:-229.44426153097533\n",
      "epoch20 reward:-129.58737168579535 avg_reward:-224.45141703871633\n",
      "epoch21 reward:-204.53098119711814 avg_reward:-223.50282485578308\n",
      "epoch22 reward:-137.0242660600593 avg_reward:-219.5719812741593\n",
      "epoch23 reward:-347.4073234498969 avg_reward:-225.13003962962617\n",
      "epoch24 reward:-348.67643872389476 avg_reward:-230.27780625855397\n",
      "epoch25 reward:-240.87895039109372 avg_reward:-230.70185202385557\n",
      "epoch26 reward:-135.9710865034781 avg_reward:-227.0583610423026\n",
      "epoch27 reward:-279.917700759318 avg_reward:-229.01611436515503\n",
      "epoch28 reward:-141.35478587735673 avg_reward:-225.88535263344792\n",
      "epoch29 reward:-478.6459197628889 avg_reward:-234.6012342586011\n",
      "epoch30 reward:-86.41949788102353 avg_reward:-229.66184304601518\n",
      "epoch31 reward:-294.12416345626514 avg_reward:-231.74127273666838\n",
      "epoch32 reward:-146.17660756634854 avg_reward:-229.06737695009588\n",
      "epoch33 reward:-63.66742023983616 avg_reward:-224.05525704978498\n",
      "epoch34 reward:-212.67550529618399 avg_reward:-223.72055846879672\n",
      "epoch35 reward:-256.6294259854162 avg_reward:-224.66081182641443\n",
      "epoch36 reward:-280.0942634478962 avg_reward:-226.20062992701116\n",
      "epoch37 reward:-272.4143815261691 avg_reward:-227.44965024050188\n",
      "epoch38 reward:-200.08204571595613 avg_reward:-226.72945012143487\n",
      "epoch39 reward:-480.20007476208895 avg_reward:-233.22869690709268\n",
      "epoch40 reward:-25.20858146355937 avg_reward:-228.02819402100437\n",
      "epoch41 reward:-309.31423160574695 avg_reward:-230.01078030355907\n",
      "epoch42 reward:-183.22962694272832 avg_reward:-228.89694331877737\n",
      "epoch43 reward:-365.52462638870617 avg_reward:-232.0743312971478\n",
      "epoch44 reward:-173.4301827970744 avg_reward:-230.74150974032793\n",
      "epoch45 reward:-107.51465268576216 avg_reward:-228.00313513911536\n",
      "epoch46 reward:-102.33303897808874 avg_reward:-225.27117652691916\n",
      "epoch47 reward:-93.10958222518678 avg_reward:-222.45922771198866\n",
      "epoch48 reward:-97.33836572415144 avg_reward:-219.85254308724203\n",
      "epoch49 reward:-150.0937010539172 avg_reward:-218.42889324982724\n",
      "epoch50 reward:-8.440613843252834 avg_reward:-214.22912766169574\n",
      "epoch51 reward:30.35560978281211 avg_reward:-209.4333484961172\n",
      "epoch52 reward:-108.11676156910464 avg_reward:-207.48495259367462\n",
      "epoch53 reward:-205.98352504176455 avg_reward:-207.45662377194046\n",
      "epoch54 reward:-185.92393310038273 avg_reward:-207.0578702409857\n",
      "epoch55 reward:-65.92908408751121 avg_reward:-204.4918923109225\n",
      "epoch56 reward:-105.554003930241 avg_reward:-202.72514430412465\n",
      "epoch57 reward:-99.85414600096668 avg_reward:-200.92038994792892\n",
      "epoch58 reward:-184.33192462918134 avg_reward:-200.6343819251919\n",
      "epoch59 reward:30.97902304399338 avg_reward:-196.70873099351078\n",
      "epoch60 reward:-59.73364698848004 avg_reward:-194.42581292676024\n",
      "epoch61 reward:-88.5614521315368 avg_reward:-192.6903316022484\n",
      "epoch62 reward:-186.7615110297578 avg_reward:-192.59470546398242\n",
      "epoch63 reward:-236.64235230468225 avg_reward:-193.2938744614538\n",
      "epoch64 reward:-271.5738997922091 avg_reward:-194.5169998572469\n",
      "epoch65 reward:-144.74674825009805 avg_reward:-193.75130367867538\n",
      "epoch66 reward:-150.46541170506296 avg_reward:-193.09545683059034\n",
      "epoch67 reward:-321.4975004268439 avg_reward:-195.0119052424747\n",
      "epoch68 reward:-209.90858345173186 avg_reward:-195.2309740396697\n",
      "epoch69 reward:-162.85985239088353 avg_reward:-194.76182734910756\n",
      "epoch70 reward:-252.03835855311792 avg_reward:-195.58006350916486\n",
      "epoch71 reward:-144.81874995319336 avg_reward:-194.86511543091174\n",
      "epoch72 reward:-193.0999596054042 avg_reward:-194.8405993777797\n",
      "epoch73 reward:-154.10237487446676 avg_reward:-194.28254150787131\n",
      "epoch74 reward:-157.97864235459383 avg_reward:-193.79194827607026\n",
      "epoch75 reward:-176.1038858563964 avg_reward:-193.55610744380795\n",
      "epoch76 reward:-63.79058822034401 avg_reward:-191.84866640139393\n",
      "epoch77 reward:-186.67539809371243 avg_reward:-191.78148109869676\n",
      "epoch78 reward:-190.04525397698114 avg_reward:-191.7592217766235\n",
      "epoch79 reward:-247.15180461231185 avg_reward:-192.46039371125244\n",
      "epoch80 reward:-42.248255269486826 avg_reward:-190.5827419807304\n",
      "epoch81 reward:-313.440568475507 avg_reward:-192.09950527078936\n",
      "epoch82 reward:-102.76250747476026 avg_reward:-191.01002968791096\n",
      "epoch83 reward:-144.2521428465413 avg_reward:-190.4466816536776\n",
      "epoch84 reward:-75.25218661252099 avg_reward:-189.07531861747333\n",
      "epoch85 reward:-317.5265228200942 avg_reward:-190.58650925515124\n",
      "epoch86 reward:-141.61384727860184 avg_reward:-190.0170596972844\n",
      "epoch87 reward:-505.7388489310325 avg_reward:-193.64604578043094\n",
      "epoch88 reward:-122.49502780226244 avg_reward:-192.83751148522444\n",
      "epoch89 reward:-224.05639924468596 avg_reward:-193.1882855049937\n",
      "epoch90 reward:-59.33469477672904 avg_reward:-191.70102338579073\n",
      "epoch91 reward:-43.77692504075726 avg_reward:-190.07548384353763\n",
      "epoch92 reward:-166.47794143535492 avg_reward:-189.81898881736174\n",
      "epoch93 reward:-227.08647050977513 avg_reward:-190.21971442695755\n",
      "epoch94 reward:-119.93425011245645 avg_reward:-189.47199672148415\n",
      "epoch95 reward:-61.777919819392935 avg_reward:-188.12784854356738\n",
      "epoch96 reward:-126.52767443181334 avg_reward:-187.48618006323662\n",
      "epoch97 reward:-169.32424778970346 avg_reward:-187.29894364804557\n",
      "epoch98 reward:-290.3447303212618 avg_reward:-188.35043126716005\n",
      "epoch99 reward:-71.5184694688815 avg_reward:-187.17031044091482\n",
      "epoch100 reward:-244.10279597846284 avg_reward:-187.7396352962903\n",
      "epoch101 reward:-169.7421247360561 avg_reward:-187.6040401782706\n",
      "epoch102 reward:-146.6707722829778 avg_reward:-186.33998301345687\n",
      "epoch103 reward:-126.29371148588194 avg_reward:-185.6059573311125\n",
      "epoch104 reward:-185.0935511501473 avg_reward:-183.2965856728332\n",
      "epoch105 reward:-246.01874514197985 avg_reward:-184.1916196500072\n",
      "epoch106 reward:47.73694245317344 avg_reward:-181.09525267150482\n",
      "epoch107 reward:-327.2782654635174 avg_reward:-183.33092893941713\n",
      "epoch108 reward:-231.54712168235113 avg_reward:-181.05042786246912\n",
      "epoch109 reward:-264.2221286093645 avg_reward:-182.47592723218827\n",
      "epoch110 reward:-123.32556164410845 avg_reward:-180.29959654418678\n",
      "epoch111 reward:-145.28242521535134 avg_reward:-178.942390083224\n",
      "epoch112 reward:-18.736297344398537 avg_reward:-177.99358765693495\n",
      "epoch113 reward:-128.27565433941206 avg_reward:-176.89960418478245\n",
      "epoch114 reward:-195.2228036297189 avg_reward:-176.09650635172034\n",
      "epoch115 reward:-130.20482668526552 avg_reward:-175.5465319607049\n",
      "epoch116 reward:-182.66501872870134 avg_reward:-174.35742542471507\n",
      "epoch117 reward:-110.81291394772617 avg_reward:-174.190799091742\n",
      "epoch118 reward:-154.53679942009254 avg_reward:-174.6800535885915\n",
      "epoch119 reward:-61.498824021061814 avg_reward:-173.14213163615437\n",
      "epoch120 reward:29.911789668578926 avg_reward:-171.54714002261062\n",
      "epoch121 reward:-196.43355616720055 avg_reward:-171.46616577231143\n",
      "epoch122 reward:-283.65542832833034 avg_reward:-172.93247739499418\n",
      "epoch123 reward:-62.95370864977475 avg_reward:-170.0879412469929\n",
      "epoch124 reward:-66.04575174375171 avg_reward:-167.26163437719148\n",
      "epoch125 reward:-96.33144383780973 avg_reward:-165.81615931165868\n",
      "epoch126 reward:-66.48875280642116 avg_reward:-165.1213359746881\n",
      "epoch127 reward:-0.2601521610679356 avg_reward:-162.32476048870555\n",
      "epoch128 reward:-138.66609536703214 avg_reward:-162.29787358360235\n",
      "epoch129 reward:-236.7010325289897 avg_reward:-159.87842471126336\n",
      "epoch130 reward:9.09446035185735 avg_reward:-158.92328512893457\n",
      "epoch131 reward:-56.69114011580878 avg_reward:-156.54895489552996\n",
      "epoch132 reward:-188.52622954739851 avg_reward:-156.97245111534048\n",
      "epoch133 reward:-113.70409589884514 avg_reward:-157.47281787193057\n",
      "epoch134 reward:-128.58363849563767 avg_reward:-156.6318992039251\n",
      "epoch135 reward:-99.65564267719603 avg_reward:-155.0621613708429\n",
      "epoch136 reward:-99.67746458111824 avg_reward:-153.25799338217513\n",
      "epoch137 reward:-256.6693950261807 avg_reward:-153.10054351717525\n",
      "epoch138 reward:-249.6581790127594 avg_reward:-153.59630485014327\n",
      "epoch139 reward:-108.70913399785668 avg_reward:-149.88139544250095\n",
      "epoch140 reward:-75.91712906086055 avg_reward:-150.388480918474\n",
      "epoch141 reward:-29.521681551983946 avg_reward:-147.59055541793632\n",
      "epoch142 reward:-45.72395676420541 avg_reward:-146.21549871615107\n",
      "epoch143 reward:-134.00663645731316 avg_reward:-143.90031881683714\n",
      "epoch144 reward:-153.5620028183115 avg_reward:-143.70163701704953\n",
      "epoch145 reward:-93.66317999141934 avg_reward:-143.5631222901061\n",
      "epoch146 reward:-100.21853027473307 avg_reward:-143.54197720307255\n",
      "epoch147 reward:-169.69932081269747 avg_reward:-144.30787458894764\n",
      "epoch148 reward:32.32714585587388 avg_reward:-143.01121947314738\n",
      "epoch149 reward:-8.00867150588688 avg_reward:-141.5903691776671\n",
      "epoch150 reward:-152.58838022089088 avg_reward:-143.03184684144347\n",
      "epoch151 reward:-57.339276360089286 avg_reward:-143.90879570287248\n",
      "epoch152 reward:-117.20790842393487 avg_reward:-143.99970717142077\n",
      "epoch153 reward:-113.67231765634165 avg_reward:-143.07659509756655\n",
      "epoch154 reward:-109.66443676036836 avg_reward:-142.3140001341664\n",
      "epoch155 reward:-214.78644201480398 avg_reward:-143.80257371343933\n",
      "epoch156 reward:21.263115063896677 avg_reward:-142.53440252349793\n",
      "epoch157 reward:-106.23648574462267 avg_reward:-142.59822592093454\n",
      "epoch158 reward:-131.92597900708176 avg_reward:-142.07416646471353\n",
      "epoch159 reward:-95.9372710486086 avg_reward:-143.34332940563954\n",
      "epoch160 reward:-52.731950521577964 avg_reward:-143.2733124409705\n",
      "epoch161 reward:-61.01755252591364 avg_reward:-142.9978734449143\n",
      "epoch162 reward:-43.86812000594904 avg_reward:-141.56893953467622\n",
      "epoch163 reward:-93.6906591790563 avg_reward:-140.13942260341997\n",
      "epoch164 reward:-223.42927782469363 avg_reward:-139.6579763837448\n",
      "epoch165 reward:-59.41544036880386 avg_reward:-138.80466330493186\n",
      "epoch166 reward:-29.410666229286818 avg_reward:-137.59411585017412\n",
      "epoch167 reward:-69.93032092393662 avg_reward:-135.07844405514507\n",
      "epoch168 reward:-39.922637868378736 avg_reward:-133.3785845993115\n",
      "epoch169 reward:-94.40846944525089 avg_reward:-132.69407076985516\n",
      "epoch170 reward:-149.78098236870164 avg_reward:-131.671497008011\n",
      "epoch171 reward:-45.87352419248345 avg_reward:-130.68204475040392\n",
      "epoch172 reward:-0.22590858772125633 avg_reward:-128.7533042402271\n",
      "epoch173 reward:-42.36197070059649 avg_reward:-127.6359001984884\n",
      "epoch174 reward:-201.01905811972603 avg_reward:-128.0663043561397\n",
      "epoch175 reward:-41.64910932624518 avg_reward:-126.72175659083818\n",
      "epoch176 reward:121.69418449982177 avg_reward:-124.86690886363654\n",
      "epoch177 reward:-6.993121623313397 avg_reward:-123.07008609893253\n",
      "epoch178 reward:-207.0790066728569 avg_reward:-123.2404236258913\n",
      "epoch179 reward:58.150630018712405 avg_reward:-120.18739927958104\n",
      "epoch180 reward:-155.31342564458197 avg_reward:-121.31805098333201\n",
      "epoch181 reward:-41.70802314861702 avg_reward:-118.6007255300631\n",
      "epoch182 reward:-210.73421092739022 avg_reward:-119.68044256458941\n",
      "epoch183 reward:-9.197977026178087 avg_reward:-118.32990090638577\n",
      "epoch184 reward:-53.37746392072966 avg_reward:-118.11115367946786\n",
      "epoch185 reward:-88.83567960645179 avg_reward:-115.82424524733146\n",
      "epoch186 reward:-86.3390432006458 avg_reward:-115.27149720655189\n",
      "epoch187 reward:-110.15089077769427 avg_reward:-111.31561762501852\n",
      "epoch188 reward:-89.65854379717433 avg_reward:-110.98725278496762\n",
      "epoch189 reward:-42.97372602243357 avg_reward:-109.17642605274506\n",
      "epoch190 reward:-203.3869755278155 avg_reward:-110.61694886025595\n",
      "epoch191 reward:-4.978862011308351 avg_reward:-110.22896822996145\n",
      "epoch192 reward:100.77436860313522 avg_reward:-107.55644512957657\n",
      "epoch193 reward:-240.60894568284587 avg_reward:-107.69166988130728\n",
      "epoch194 reward:97.1612746163386 avg_reward:-105.52071463401933\n",
      "epoch195 reward:-150.7692478400821 avg_reward:-106.41062791422621\n",
      "epoch196 reward:-13.150806613502198 avg_reward:-105.27685923604311\n",
      "epoch197 reward:-49.029353876291054 avg_reward:-104.07391029690898\n",
      "epoch198 reward:105.35033093133978 avg_reward:-100.11695968438296\n",
      "epoch199 reward:41.91702884909144 avg_reward:-98.98260470120323\n",
      "epoch200 reward:-40.997895747033056 avg_reward:-96.95155569888894\n",
      "epoch201 reward:-21.008707594136098 avg_reward:-95.46422152746973\n",
      "epoch202 reward:-115.66756224946243 avg_reward:-95.15418942713457\n",
      "epoch203 reward:-116.3337434398457 avg_reward:-95.05458974667422\n",
      "epoch204 reward:-343.9396921076515 avg_reward:-96.64305115624924\n",
      "epoch205 reward:-75.82609112645926 avg_reward:-94.94112461609404\n",
      "epoch206 reward:161.20604122530642 avg_reward:-93.8064336283727\n",
      "epoch207 reward:43.6489023466109 avg_reward:-90.09716195027141\n",
      "epoch208 reward:-59.03451256237102 avg_reward:-88.37203585907162\n",
      "epoch209 reward:-81.56656011004007 avg_reward:-86.5454801740784\n",
      "epoch210 reward:-50.58791420820498 avg_reward:-85.81810369971936\n",
      "epoch211 reward:-19.512179663929913 avg_reward:-84.56040124420515\n",
      "epoch212 reward:-17.406968222931695 avg_reward:-84.54710795299046\n",
      "epoch213 reward:-86.85684350824715 avg_reward:-84.1329198446788\n",
      "epoch214 reward:-56.12212281040756 avg_reward:-82.74191303648571\n",
      "epoch215 reward:-36.98717973315853 avg_reward:-81.80973656696465\n",
      "epoch216 reward:14.774555454069057 avg_reward:-79.83534082513694\n",
      "epoch217 reward:68.84033128369268 avg_reward:-78.03880837282276\n",
      "epoch218 reward:-48.59373467630185 avg_reward:-76.97937772538485\n",
      "epoch219 reward:70.16195793520222 avg_reward:-75.6627699058222\n",
      "epoch220 reward:57.58076829864042 avg_reward:-75.38608011952158\n",
      "epoch221 reward:140.87510297129293 avg_reward:-72.01299352813665\n",
      "epoch222 reward:-31.210670070421525 avg_reward:-69.48854594555756\n",
      "epoch223 reward:-15.328752017263284 avg_reward:-69.01229637923245\n",
      "epoch224 reward:74.86454618323293 avg_reward:-67.6031933999626\n",
      "epoch225 reward:61.539018285801994 avg_reward:-66.02448877872648\n",
      "epoch226 reward:87.42346450344596 avg_reward:-64.48536660562782\n",
      "epoch227 reward:-251.80554453289852 avg_reward:-67.00082052934611\n",
      "epoch228 reward:61.30275852498511 avg_reward:-65.00113199042595\n",
      "epoch229 reward:-64.05544185774895 avg_reward:-63.27467608371354\n",
      "epoch230 reward:14.14818823053885 avg_reward:-63.224138804926724\n",
      "epoch231 reward:-34.449841298224925 avg_reward:-63.00172581675088\n",
      "epoch232 reward:-52.502364831144746 avg_reward:-61.64148716958834\n",
      "epoch233 reward:-231.6231249646505 avg_reward:-62.8206774602464\n",
      "epoch234 reward:-235.29922757757902 avg_reward:-63.88783335106581\n",
      "epoch235 reward:-72.7122099815385 avg_reward:-63.61839902410923\n",
      "epoch236 reward:-144.56951604454662 avg_reward:-64.06731953874352\n",
      "epoch237 reward:91.47069314599523 avg_reward:-60.585918657021764\n",
      "epoch238 reward:59.63058478213194 avg_reward:-57.49303101907285\n",
      "epoch239 reward:-53.45113080023585 avg_reward:-56.94045098709664\n",
      "epoch240 reward:-111.28044720211221 avg_reward:-57.29408416850916\n",
      "epoch241 reward:-28.40427796953803 avg_reward:-57.2829101326847\n",
      "epoch242 reward:33.12464181430421 avg_reward:-56.49442414689961\n",
      "epoch243 reward:235.9246439512959 avg_reward:-52.79511134281351\n",
      "epoch244 reward:106.16977860885014 avg_reward:-50.197793528541894\n",
      "epoch245 reward:50.19883583451228 avg_reward:-48.75917337028258\n",
      "epoch246 reward:121.36022790579088 avg_reward:-46.543385788477345\n",
      "epoch247 reward:63.83767345511748 avg_reward:-44.2080158457992\n",
      "epoch248 reward:12.948350547183473 avg_reward:-44.40180379888611\n",
      "epoch249 reward:41.49665717422896 avg_reward:-43.90675051208493\n",
      "epoch250 reward:68.6039818133973 avg_reward:-41.69482689174205\n",
      "epoch251 reward:-36.088603457584995 avg_reward:-41.482320162717016\n",
      "epoch252 reward:-16.79620206172979 avg_reward:-40.47820309909497\n",
      "epoch253 reward:68.22257483334234 avg_reward:-38.65925417419812\n",
      "epoch254 reward:35.45616875732662 avg_reward:-37.20804811902117\n",
      "epoch255 reward:-62.384479949977326 avg_reward:-35.684028498372896\n",
      "epoch256 reward:-83.6187522577407 avg_reward:-36.73284717158928\n",
      "epoch257 reward:-79.25711088001287 avg_reward:-36.46305342294317\n",
      "epoch258 reward:-63.81430643089398 avg_reward:-35.7819366971813\n",
      "epoch259 reward:-104.16708893257146 avg_reward:-35.86423487602092\n",
      "epoch260 reward:7.193269031806949 avg_reward:-35.26498268048707\n",
      "epoch261 reward:-24.819371078727787 avg_reward:-34.90300086601521\n",
      "epoch262 reward:-72.25384827019548 avg_reward:-35.186858148657684\n",
      "epoch263 reward:-186.16604149859012 avg_reward:-36.111611971853016\n",
      "epoch264 reward:-97.5009894196193 avg_reward:-34.85232908780228\n",
      "epoch265 reward:-273.84937929411814 avg_reward:-36.996668477055415\n",
      "epoch266 reward:-38.97368332229059 avg_reward:-37.09229864798546\n",
      "epoch267 reward:-241.9722405432573 avg_reward:-38.812717844178664\n",
      "epoch268 reward:-75.59336809106165 avg_reward:-39.1694251464055\n",
      "epoch269 reward:184.85051624458262 avg_reward:-36.37683528950716\n",
      "epoch270 reward:-66.70869344262046 avg_reward:-35.54611240024635\n",
      "epoch271 reward:-38.044370783707 avg_reward:-35.46782086615858\n",
      "epoch272 reward:-281.4705520276575 avg_reward:-38.28026730055796\n",
      "epoch273 reward:246.98480900035108 avg_reward:-35.386799503548474\n",
      "epoch274 reward:-49.38355063440781 avg_reward:-33.87044442869529\n",
      "epoch275 reward:10.156355347805574 avg_reward:-33.35238978195478\n",
      "epoch276 reward:-123.77129008082022 avg_reward:-35.80704452776121\n",
      "epoch277 reward:-40.318894323376185 avg_reward:-36.14030225476183\n",
      "epoch278 reward:58.13792756163453 avg_reward:-33.488132912416916\n",
      "epoch279 reward:-55.17566526402324 avg_reward:-34.621395865244274\n",
      "epoch280 reward:-16.551109739835866 avg_reward:-33.23377270619682\n",
      "epoch281 reward:-40.81238563488898 avg_reward:-33.22481633105953\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[104], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch, maxt, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[0;32m     12\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(maxt):\n\u001b[1;32m---> 15\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     17\u001b[0m     agent\u001b[38;5;241m.\u001b[39mstep(state, action ,reward, next_state, done)\n",
      "Cell \u001b[1;32mIn[102], line 41\u001b[0m, in \u001b[0;36mDQNAgent.choose_action\u001b[1;34m(self, state, epsilon)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose_action\u001b[39m(\u001b[38;5;28mself\u001b[39m,state,epsilon):\n\u001b[1;32m---> 41\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
