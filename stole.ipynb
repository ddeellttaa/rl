{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b32258",
   "metadata": {},
   "source": [
    "# Double Dueling Deep Q-Network (DQN) using PyTorch\n",
    "Environment: LunarLander-v2\n",
    "\n",
    "### Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "555ed7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312da450",
   "metadata": {},
   "source": [
    "## Specify the Environment, and Explore the State and Action Spaces\n",
    "* Let's begin with an initializing the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05fa0d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space:  Box(8,)\n",
      "State shape:  (8,)\n",
      "Action space:  Discrete(4)\n",
      "Number of actions:  4\n"
     ]
    }
   ],
   "source": [
    "# Create an environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(0);\n",
    "print('State space: ', env.observation_space)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "\n",
    "print('Action space: ', env.action_space)\n",
    "print('Number of actions: ', env.action_space.n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b9996",
   "metadata": {},
   "source": [
    "### Implement Q-Network\n",
    "Building the Network: Actor (policy) Model\n",
    "input_size = state_size\n",
    "output_size = action_size\n",
    "using same seed\n",
    "hidden_layers: fc1, fc2\n",
    "\n",
    "<!-- Define Layer of model: [FC-RELU-FC-RELU-FC] -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "514d85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        '''\n",
    "        Builds a feedforward nework with two hidden layers\n",
    "        Initialize parameters\n",
    "        \n",
    "        Params\n",
    "        =========\n",
    "        state_size (int): Dimension of each state (input_size)\n",
    "        action_size (int): dimension of each action (output_size)\n",
    "        seed (int): Random seed(using 0)\n",
    "        fc1_units (int): Size of the first hidden layer\n",
    "        fc2_units (int): Size of the second hidden layer\n",
    "        '''\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        # Add the first laer, input to hidden layer\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        # Add more hidden layer\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "\n",
    "        # State-value V\n",
    "        self.V = nn.Linear(fc2_units, 1)\n",
    "        \n",
    "        # Advantage function A\n",
    "        self.A = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass through the network. Build a network that mps state -> action values.\n",
    "        \n",
    "        return Q function\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        V = self.V(x)\n",
    "        A = self.A(x)\n",
    "        \n",
    "        return V + (A - A.mean(dim=1, keepdim=True))\n",
    "    \n",
    "    \n",
    "# with out Dueling\n",
    "# class QNetwork(nn.Module):\n",
    "#     def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "#         '''\n",
    "#         Builds a feedforward nework with two hidden layers\n",
    "#         Initialize parameters\n",
    "        \n",
    "#         Params\n",
    "#         =========\n",
    "#         state_size (int): Dimension of each state (input_size)\n",
    "#         action_size (int): dimension of each action (output_size)\n",
    "#         seed (int): Random seed(using 0)\n",
    "#         fc1_units (int): Size of the first hidden layer\n",
    "#         fc2_units (int): Size of the second hidden layer\n",
    "#         '''\n",
    "#         super(QNetwork, self).__init__()\n",
    "#         self.seed = torch.manual_seed(seed)\n",
    "#         # Add the first laer, input to hidden layer\n",
    "#         self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "#         # Add more hidden layer\n",
    "#         self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "#         self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "#     def forward(self, state):\n",
    "#         \"\"\"\n",
    "#         Forward pass through the network. Build a network that mps state -> action values.\n",
    "#         \"\"\"\n",
    "#         x = F.relu(self.fc1(state))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         return self.fc3(x)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d9ceab",
   "metadata": {},
   "source": [
    "# Experience Replay\n",
    "\n",
    "### Set finite memory size N\n",
    "Algorithm only stores the last N experience tuples in the replay memory, and sample uniformly at random from D when performing updates. The memory buffer does not differentiate imporant transitions and always overwrites with recent transitions owing to the finitre memory size N.\n",
    "\n",
    "Uniform sampling gives equal importance to all transitions in the replay memory\n",
    "\n",
    "We store each experienced tuple in the buffer as we are interacting with the environment and then sample a small bath of tuples from it in order to learn. Therefore, we are able to learn from individual tuples multiple times\n",
    "\n",
    "Sequential order runs the risk of getting swayed by the effect of the correlations.\n",
    "\n",
    "With experience replay, can sample from this buffer at random\n",
    "\n",
    "Randomizing the samples breaks these correlations and therefore reduces the variance of the updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "189dba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        '''\n",
    "        Only stroes the last N experience tuples in the replay memory\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        '''\n",
    "        # Initialize replay memory\n",
    "        self.acion_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size) # set N memory size\n",
    "        self.batch_size = batch_size\n",
    "        # build named experience tuples\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        '''\n",
    "        we store the agent's experiences at each time-step, e_t = (s_t,a_t,r_t,s_(t+1))\n",
    "        '''\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        '''\n",
    "        Samples uniformly at random from D(D_t = {e_1,...,e_t}) when  performing updates\n",
    "        '''\n",
    "        # D\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        #store in\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device) # gpu\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        # return D\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Return the current size of internal memory\n",
    "        '''\n",
    "        return len(self.memory)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbcb7fe",
   "metadata": {},
   "source": [
    "### Implement agent\n",
    "* Agent(state_size=8, action_size=4, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dd74428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# hyperparameters\n",
    "LR = 5e-4                # learning rate\n",
    "BUFFER_SIZE = int(1e5)   # replay buffer size N\n",
    "BATCH_SIZE = 64          # minibatch size\n",
    "UPDATE_EVERY = 4         # how often to update the network\n",
    "GAMMA = 0.99             # Discount factor\n",
    "TAU = 1e-3               # for soft update of target parameters\n",
    "\n",
    "\n",
    "# Setup Gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Build Agent(): Evaluate our agent on unmodified games (dqn agent)\n",
    "class Agent():\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, seed): #8, 4, 0\n",
    "        \"\"\"\n",
    "        Initialize an Agent object.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state = 8\n",
    "            action_size (int): dimension of each action = 4\n",
    "            seed (int): random seed = 0\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # Q-Network: Neural network function approx. with weights theta θ as a Q-Network.\n",
    "        # A Q-Network can be trained by adjusting the parameters θ_i at iteration i to reduce the mse in the Bellman equation\n",
    "        # The outputs correspond to the predicted Q-values of the individual action for input state\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device) # gpu\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        # specify optimizer(Adam)\n",
    "        # optim.Adam(Qnet.parameters(), small learning rate)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR) ###\n",
    "        \n",
    "        # First, use a technique known as experience replay in which we stre the agent's experience at each time-step,\n",
    "        # e_t= (s_t, a_t, r_t, s_(t_1)), in a data set D_t ={e_1,...,e_t},pooled over many episodes(where the end of an episode occurs when\n",
    "        # a terminal state is reached) into a replay memory.\n",
    "        #Initialize replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed) ###\n",
    "        # Initialize time step (update every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps\n",
    "        self.t_step =(self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # if enough samples are availabe in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE: ###\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA) ###\n",
    "                \n",
    "    def act(self, state, eps=0):\n",
    "        '''\n",
    "        Choose action A from state S using policy pi <- epsilon-Greedt(q^hat (S,A,w))\n",
    "        Return actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection        \n",
    "        '''\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        # It is off-policy: it learns about the greedy policy a = argmax Q(s,a';θ),\n",
    "        # while following a behaviour distribution is often selected by an eps-greedy policy\n",
    "        # that follows the greey policy with probability 1-eps and selects a random action\n",
    "        # with probability eps.        \n",
    "        # Epsilon-greedy action selection\n",
    "        # \n",
    "        # with probability epsilon select a random action a_t\n",
    "        # otherwise select a_t = argmax_a Q (phi(s_t),a; θ)\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        \n",
    "    def learn(self, experiences, gamma): #----only use the local and target Q-networks to compute the loss before taking a step towards minimizing the loss\n",
    "        '''\n",
    "        Update value parameters using given batch of experience tuples\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \n",
    "        '''\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        #####DQN\n",
    "        ## Get max predicted Q values (for next states) from target model\n",
    "        #Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        #\n",
    "        ## Compute Q targets for current states\n",
    "        #Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        #\n",
    "        ## Get expected Q values from local model\n",
    "        #Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        ##### Double DQN\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            # fetch max action arguemnt to pass\n",
    "            Q_pred = self.qnetwork_local(next_states)\n",
    "            max_actions = torch.argmax(Q_pred, dim=1).long().unsqueeze(1)\n",
    "            # Q_targets over next statesfrom actions will be taken based on Q_pred's max_action\n",
    "            Q_next = self.qnetwork_target(next_states)\n",
    "        self.qnetwork_local.train()\n",
    "        Q_targets = rewards + (gamma * Q_next.gather(1, max_actions) * (1.0 - dones))\n",
    "        ## Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        \n",
    "        ###############\n",
    "        # apply loss fucntion\n",
    "        # calculate the loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # zero the parameter (weight) gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # backward pass to calculate the parameter gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the parameters\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        #################\n",
    "        #Update target network\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU) ###\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97d648e",
   "metadata": {},
   "source": [
    "# Watch an untrained agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dbd6b6",
   "metadata": {},
   "source": [
    "# Train the Agent with DQN\n",
    "Train Deep Q-Learning with\n",
    "* maximum number of training episodes: 2000\n",
    "* maximum number of timesteps per episode: 1000\n",
    "* starting value of epsilon for epsilon-greedy action selection: 1.0\n",
    "* minimum value of epsilon: 0.01\n",
    "* multiplacative factor (per episode for decreasing epsilon: 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91021fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\t Average Score:-173.911\n",
      "Episode 200\t Average Score:-97.4881\n",
      "Episode 284\t Average Score: -44.32"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m#double deuling dqn\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mdqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# plot the scores\u001b[39;00m\n\u001b[0;32m     69\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
      "Cell \u001b[1;32mIn[10], line 39\u001b[0m, in \u001b[0;36mdqn\u001b[1;34m(n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[0;32m     37\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# agent performs internal updates based on sampled experience\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# update the sampled reward\u001b[39;00m\n\u001b[0;32m     41\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[8], line 66\u001b[0m, in \u001b[0;36mAgent.step\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m BATCH_SIZE: \u001b[38;5;66;03m###\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGAMMA\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 144\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self, experiences, gamma)\u001b[0m\n\u001b[0;32m    141\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# update the parameters\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m#################\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m#Update target network\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoft_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnetwork_local, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnetwork_target, TAU)\n",
      "File \u001b[1;32md:\\install\\anaconda\\envs\\vk\\lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32md:\\install\\anaconda\\envs\\vk\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32md:\\install\\anaconda\\envs\\vk\\lib\\site-packages\\torch\\optim\\adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    155\u001b[0m         group,\n\u001b[0;32m    156\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m         state_steps)\n\u001b[1;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\install\\anaconda\\envs\\vk\\lib\\site-packages\\torch\\optim\\adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\install\\anaconda\\envs\\vk\\lib\\site-packages\\torch\\optim\\adam.py:370\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    366\u001b[0m         (param\u001b[38;5;241m.\u001b[39mis_cuda \u001b[38;5;129;01mand\u001b[39;00m step_t\u001b[38;5;241m.\u001b[39mis_cuda) \u001b[38;5;129;01mor\u001b[39;00m (param\u001b[38;5;241m.\u001b[39mis_xla \u001b[38;5;129;01mand\u001b[39;00m step_t\u001b[38;5;241m.\u001b[39mis_xla)\n\u001b[0;32m    367\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf capturable=True, params and state_steps must be CUDA or XLA tensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# update step\u001b[39;00m\n\u001b[1;32m--> 370\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    373\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#double dueling dqn\n",
    "def dqn(n_episodes = 2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"\n",
    "    Train the Agent with Deep Q-Learning\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    # Initialize collecting scores from each episode\n",
    "    scores = []\n",
    "    # Initialize collecting maxlen(100) scores\n",
    "    scores_window = deque(maxlen=100)\n",
    "    # initialize starting value of epsilon\n",
    "    eps = eps_start\n",
    "    \n",
    "    # for each episode----------------\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # begin the episode\n",
    "        state = env.reset()\n",
    "        # initialize the sampled score(reward)\n",
    "        score = 0\n",
    "        \n",
    "        # Set constrain maximum number of time step per episode\n",
    "        for t in range(max_t):\n",
    "            # agent select an action\n",
    "            action = agent.act(state, eps)\n",
    "            # agent performs the selected action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # agent performs internal updates based on sampled experience\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            # update the sampled reward\n",
    "            score += reward\n",
    "            # update the state (s <- s') to next time step\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        #Save most recent score\n",
    "        scores_window.append(score)\n",
    "        #save most recent score\n",
    "        scores.append(score)\n",
    "        #Decrease epsilon\n",
    "        eps = max(eps_end, eps_decay*eps)\n",
    "        \n",
    "        # monitor progress\n",
    "        print('\\rEpisode {}\\t Average Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        # get average reward from last 100 episodes\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\t Average Score:{:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=200.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100,\n",
    "                                                                                         np.mean(scores_window)))\n",
    "            # save model\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "#double deuling dqn\n",
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b25cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the weights from file\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "for i in range(7):\n",
    "    state = env.reset()\n",
    "    img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    for j in range(500):\n",
    "        action = agent.act(state)\n",
    "        img.set_data(env.render(mode='rgb_array')) \n",
    "        plt.axis('off')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ceb894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0bd6de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b601e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
